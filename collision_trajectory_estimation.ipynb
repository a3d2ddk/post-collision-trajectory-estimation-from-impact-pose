{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSTtGnTA5Z3puF0KGqvkDJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a3d2ddk/post-collision-trajectory-estimation-from-impact-pose/blob/main/collision_trajectory_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f3757c-6d02-4932-e0b6-1d9499cc2c82",
        "id": "ORffBjJdQGk1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('/content/repo'):\n",
        "  print('Repository already cloned!')\n",
        "else:\n",
        "  print('Cloning repository...')\n",
        "  !git clone --quiet https://github.com/a3d2ddk/post-collision-trajectory-estimation-from-impact-pose.git\n",
        "  !mv post-collision-trajectory-estimation-from-impact-pose repo\n",
        "  print('Repository cloned!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "need_pytorch3d=False\n",
        "\n",
        "try:\n",
        "  import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "  need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "  os.chdir('/content/drive/MyDrive/pytorch3d_wheels')\n",
        "\n",
        "  wheel_files = glob.glob('*.whl')\n",
        "  wheel = max(wheel_files, key=os.path.getmtime)\n",
        "  print(\"Latest created wheel:\", wheel)\n",
        "\n",
        "  # We try to install PyTorch3D from repo wheel.\n",
        "  !pip install {wheel}\n",
        "  #!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
        "\n",
        "  print('Packages installed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb3b95c-d89f-4c6c-f3e6-7bc5182b9dd3",
        "id": "2rpgyUAiQKmw"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "dataset_dir = '/content/repo/Datasets/run_0003'\n",
        "frame_dir = dataset_dir + '/rgb'\n",
        "seg_dir = dataset_dir + '/instance_seg'\n",
        "\n",
        "num_frames = 180\n",
        "\n",
        "frames = []\n",
        "segs = []\n",
        "\n",
        "for i in range(num_frames):\n",
        "  str_num = str(i).zfill(4)\n",
        "\n",
        "  frames.append(np.array(Image.open(frame_dir + '/rgb_' + str_num + '.png')))\n",
        "  seg_i = np.load((seg_dir + '/instance_seg_' + str_num + '.npy'), allow_pickle=True).item()\n",
        "  segs.append(seg_i['data'])\n",
        "\n",
        "#print(frames[0].shape)\n",
        "#print(segs[0].shape)"
      ],
      "metadata": {
        "id": "Rmap3eCvbuh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#non_zero_count = np.count_nonzero(segs[0] == 2)\n",
        "#print(non_zero_count)\n",
        "\n",
        "images = []\n",
        "final_cube1 = []\n",
        "final_cube2 = []\n",
        "\n",
        "zs = np.zeros_like(frames[0])\n",
        "\n",
        "for i in range(num_frames):\n",
        "  starting_img = frames[i]\n",
        "  seg_map = segs[i]\n",
        "\n",
        "  images.append(Image.fromarray(starting_img))\n",
        "\n",
        "  fin_img1 = zs.copy()\n",
        "  fin_img1[seg_map == 2] = starting_img[seg_map == 2]\n",
        "  final_cube1.append(Image.fromarray(fin_img1))\n",
        "\n",
        "  fin_img2 = zs.copy()\n",
        "  fin_img2[seg_map == 3] = starting_img[seg_map == 3]\n",
        "  final_cube2.append(Image.fromarray(fin_img2))\n",
        "\n",
        "#print(final_cube1[0].shape)"
      ],
      "metadata": {
        "id": "M-NYBbG8ULC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "images[0].save('original.gif',save_all = True, append_images = images[1:], duration = num_frames / 3, loop = 0)\n",
        "final_cube1[0].save('cube1.gif',save_all = True, append_images = final_cube1[1:], duration = num_frames / 3, loop = 0)\n",
        "final_cube2[0].save('cube2.gif',save_all = True, append_images = final_cube2[1:], duration = num_frames / 3, loop = 0)\n",
        "\n",
        "start = Image.open('original.gif')\n",
        "seg1 = Image.open('cube1.gif')\n",
        "seg2 = Image.open('cube2.gif')"
      ],
      "metadata": {
        "id": "KkzpMv4_T0fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "!git clone --recursive https://github.com/naver/mast3r\n",
        "\n",
        "os.chdir('/content/mast3r')\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r dust3r/requirements.txt\n",
        "!pip install -r dust3r/requirements_optional.txt"
      ],
      "metadata": {
        "id": "vXPwAk0amFYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mast3r.model import AsymmetricMASt3R\n",
        "from mast3r.fast_nn import fast_reciprocal_NNs\n",
        "\n",
        "import mast3r.utils.path_to_dust3r\n",
        "from dust3r.inference import inference\n",
        "from dust3r.utils.image import load_images\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms.functional\n",
        "from matplotlib import pyplot as pl\n",
        "\n",
        "import cv2\n",
        "\n",
        "from typing import Literal, Tuple\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.renderer.cameras import CamerasBase, PerspectiveCameras, FoVPerspectiveCameras\n",
        "\n",
        "import math\n",
        "\n",
        "from pytorch3d.renderer import (\n",
        "    MeshRenderer, MeshRasterizer, SoftPhongShader,\n",
        "    RasterizationSettings, PointLights, PerspectiveCameras, look_at_view_transform\n",
        ")\n",
        "\n",
        "# ---------------------------- IMPORTS -----------------------------------------\n",
        "# Stdlib\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Literal, Dict, Any\n",
        "\n",
        "# Third-party\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import imageio\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# PyTorch3D — IO & data structures\n",
        "from pytorch3d.io import load_obj, load_ply, load_objs_as_meshes\n",
        "from pytorch3d.structures import Meshes\n",
        "\n",
        "# PyTorch3D — transforms\n",
        "from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "# PyTorch3D — rendering\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    look_at_view_transform,\n",
        "    look_at_rotation,\n",
        "    camera_position_from_spherical_angles,\n",
        "    RasterizationSettings,\n",
        "    MeshRenderer,\n",
        "    MeshRasterizer,\n",
        "    BlendParams,\n",
        "    SoftSilhouetteShader,\n",
        "    SoftPhongShader,\n",
        "    HardPhongShader,\n",
        "    PointLights,\n",
        "    DirectionalLights,\n",
        "    Materials,\n",
        "    TexturesUV,\n",
        "    TexturesVertex,\n",
        ")\n",
        "from pytorch3d.renderer.cameras import CamerasBase\n",
        "\n",
        "# PyTorch3D — visualization helpers (optional)\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n"
      ],
      "metadata": {
        "id": "xwbCwTJBn2br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unprojector:\n",
        "    @staticmethod\n",
        "    def bilinear_sample_depth(depth: torch.Tensor, uv):\n",
        "        \"\"\"Bilinear sample depth map at pixel (u,v).\"\"\"\n",
        "        H, W = depth.shape\n",
        "        u, v = uv\n",
        "        u = np.clip(u, 0, W - 1)\n",
        "        v = np.clip(v, 0, H - 1)\n",
        "\n",
        "        u0, v0 = int(np.floor(u)), int(np.floor(v))\n",
        "        u1, v1 = min(u0+1, W-1), min(v0+1, H-1)\n",
        "        du, dv = u - u0, v - v0\n",
        "\n",
        "        d00 = depth[v0, u0]\n",
        "        d10 = depth[v0, u1]\n",
        "        d01 = depth[v1, u0]\n",
        "        d11 = depth[v1, u1]\n",
        "\n",
        "        d0 = d00 * (1 - du) + d10 * du\n",
        "        d1 = d01 * (1 - du) + d11 * du\n",
        "        return float((d0 * (1 - dv) + d1 * dv).item())\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def unproject_points(pts_uv, depths, fx, fy, cx, cy, R, T):\n",
        "        \"\"\"Convert pixels+depth to 3D in camera and world coords.\"\"\"\n",
        "        fx = -fx\n",
        "        fy = -fy\n",
        "        Kinv = np.linalg.inv(np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=np.float64))\n",
        "        X_cam_list = []\n",
        "        for (u,v), z in zip(pts_uv, depths):\n",
        "            if z <= 0:\n",
        "                X_cam_list.append([np.nan]*3)\n",
        "                continue\n",
        "            pix = np.array([u, v, 1.0], dtype=np.float64)\n",
        "            ray = Kinv @ pix\n",
        "            # ray = (Kinv @ pix.T).T\n",
        "            X_cam_list.append((ray * z).tolist())\n",
        "        X_cam = np.array(X_cam_list)\n",
        "        R_np = R.detach().cpu().numpy()[0]\n",
        "        T_np = T.detach().cpu().numpy()[0]\n",
        "        X_world = (X_cam - T_np) @ R_np\n",
        "        return X_cam, X_world\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def recover_3D_points(points_uv, depth, fx, fy, cx, cy, R, T):\n",
        "\n",
        "        # Get the interpolated depths for the list of (u,v) points\n",
        "        depths = [Unprojector.bilinear_sample_depth(depth, uv) for uv in points_uv]\n",
        "\n",
        "        # Unproject the interpolated (u,v) points to obtain 3-D coordinates\n",
        "        X_cam, X_world = Unprojector.unproject_points(points_uv, depths, fx, fy, cx, cy, R, T)\n",
        "\n",
        "        return X_cam, X_world, depths\n",
        "\n",
        "    @staticmethod\n",
        "    def list_recovered_3D_points(points_uv, depth, fx, fy, cx, cy, R, T):\n",
        "\n",
        "        # depths = [Unprojector.bilinear_sample_depth(depth, uv) for uv in points_uv]\n",
        "\n",
        "        # X_cam, X_world = Unprojector.unproject_points(points_uv, depths, fx, fy, cx, cy, R, T)\n",
        "\n",
        "        X_cam, X_world, depths = Unprojector.recover_3D_points(points_uv, depth, fx, fy, cx, cy, R, T)\n",
        "\n",
        "        for (u,v), z, xc, xw in zip(points_uv, depths, X_cam, X_world):\n",
        "            print(f\"Pixel ({u},{v}) -> Z={z:.4f}, \"\n",
        "                  f\"Cam=({xc[0]:.4f},{xc[1]:.4f},{xc[2]:.4f}), \"\n",
        "                  f\"World=({xw[0]:.4f},{xw[1]:.4f},{xw[2]:.4f})\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_uvd_depth_map(depth_map, points_uv, W, H):\n",
        "        \"\"\"\n",
        "        Get the interpolated depths and create the list (W-u, H-v, depth) to serve as\n",
        "        input to pytorch3d unproject()\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the interpolated depths for the list of (u,v) points\n",
        "        depths = [Unprojector.bilinear_sample_depth(depth_map.cpu(), uv) for uv in points_uv]\n",
        "\n",
        "        # Create the (u,v,d) to pass to unproject where d = depth\n",
        "        zz = np.array([\n",
        "          depths\n",
        "        ], dtype=np.float32).T\n",
        "\n",
        "        # Convert from list to np.array\n",
        "        uv = np.array([\n",
        "          points_uv\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Pick depths from depth map and invert them to be in camera coordinates\n",
        "        depth_cam = 1.0/zz\n",
        "\n",
        "        # Concatenate (u,v) and depth to form (u,v,depth)\n",
        "        uvd = np.concatenate([uv.squeeze(), depth_cam], axis=1)\n",
        "\n",
        "        # Flip image axes using image size (2-D flip, not 3-D)\n",
        "        uvd[:,0] = -uvd[:,0] + W\n",
        "        uvd[:,1] = -uvd[:,1] + H\n",
        "\n",
        "        return uvd\n",
        "\n",
        "\n",
        "#----------------------------------- CAM ---------------------------------------\n",
        "class Cam:\n",
        "\n",
        "    @staticmethod\n",
        "    def add_camera_roll_to_RT(R, T, roll_deg, *, device=None, mode=\"camera\"):\n",
        "        \"\"\"\n",
        "        Compose a Z-axis roll into (R,T), keeping the same camera center C.\n",
        "        Grad-safe: roll_deg can be a Tensor/Parameter.\n",
        "        mode: \"camera\" -> R' = Rz @ R ; \"world\" -> R' = R @ Rz\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(R): R = torch.as_tensor(R)\n",
        "        if not torch.is_tensor(T): T = torch.as_tensor(T)\n",
        "        dev   = device or R.device\n",
        "        dtype = torch.float32\n",
        "        R = R.to(dev, dtype)\n",
        "        T = T.to(dev, dtype)\n",
        "\n",
        "        unbatched = (R.ndim == 2)\n",
        "        if unbatched:\n",
        "            R = R[None, ...]\n",
        "            T = T[None, ...]\n",
        "\n",
        "        # C from T = -R^T C  =>  C = -R T\n",
        "        C = -torch.matmul(R, T[..., None]).squeeze(-1)\n",
        "\n",
        "        theta = torch.as_tensor(roll_deg, dtype=dtype, device=dev).reshape(1)  # keep grad\n",
        "        c, s = torch.cos(torch.deg2rad(theta)), torch.sin(torch.deg2rad(theta))\n",
        "        z = torch.zeros_like(c); o = torch.ones_like(c)\n",
        "        Rz = torch.stack([\n",
        "            torch.stack([ c, -s, z], dim=-1),\n",
        "            torch.stack([ s,  c, z], dim=-1),\n",
        "            torch.stack([ z,  z,  o], dim=-1),\n",
        "        ], dim=1)  # (1,3,3)\n",
        "\n",
        "        R_new = torch.matmul(Rz, R) if mode == \"camera\" else torch.matmul(R, Rz)\n",
        "        T_new = -torch.matmul(R_new.transpose(1, 2), C[..., None]).squeeze(-1)\n",
        "\n",
        "        if unbatched:\n",
        "            R_new, T_new = R_new[0], T_new[0]\n",
        "        return R_new, T_new\n",
        "\n",
        "#----------------------------------- Util ---------------------------------------\n",
        "class Util:\n",
        "    @staticmethod\n",
        "    def clear_cuda_cache():\n",
        "        import gc\n",
        "        gc.collect()                 # clear Python refs\n",
        "        torch.cuda.empty_cache()     # release cached blocks to the driver\n",
        "        torch.cuda.ipc_collect()     # (optional) clean up inter-proc handles\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_cuda_cache_new(all_devices: bool = True, do_ipc_collect: bool = True) -> bool:\n",
        "        \"\"\"\n",
        "        Clears PyTorch CUDA cache (and optionally IPC handles).\n",
        "        Returns True if CUDA was available and cleared, else False.\n",
        "\n",
        "        Args:\n",
        "            all_devices: if True, clear cache on every visible GPU; if False, only current device.\n",
        "            do_ipc_collect: also call torch.cuda.ipc_collect() to clean up inter-process handles.\n",
        "        \"\"\"\n",
        "        import gc, torch\n",
        "        gc.collect()  # clear Python refs\n",
        "\n",
        "        if not torch.cuda.is_available():\n",
        "            return False\n",
        "\n",
        "        if all_devices and torch.cuda.device_count() > 1:\n",
        "            current = torch.cuda.current_device()\n",
        "            for i in range(torch.cuda.device_count()):\n",
        "                with torch.cuda.device(i):\n",
        "                    torch.cuda.empty_cache()\n",
        "                    if do_ipc_collect and hasattr(torch.cuda, \"ipc_collect\"):\n",
        "                        torch.cuda.ipc_collect()\n",
        "            torch.cuda.set_device(current)\n",
        "        else:\n",
        "            torch.cuda.empty_cache()\n",
        "            if do_ipc_collect and hasattr(torch.cuda, \"ipc_collect\"):\n",
        "                torch.cuda.ipc_collect()\n",
        "\n",
        "        return True\n",
        "\n",
        "#----------------------------------- Image ---------------------------------------\n",
        "class ImageProcessor:\n",
        "\n",
        "    @staticmethod\n",
        "    def alpha_over_rgba(\n",
        "        background_rgba: np.ndarray,\n",
        "        overlay_rgba: np.ndarray,\n",
        "        resize_overlay: bool = False,\n",
        "        premultiplied: bool = False\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Composite overlay_rgba OVER background_rgba (both uint8 RGBA).\n",
        "        Returns uint8 RGBA.\n",
        "\n",
        "        Args:\n",
        "            background_rgba: (H,W,4) uint8\n",
        "            overlay_rgba:    (h,w,4) uint8\n",
        "            resize_overlay:  if True, resize overlay to background size\n",
        "            premultiplied:   set True if images are premultiplied alpha; defaults to straight alpha\n",
        "        \"\"\"\n",
        "        if background_rgba.ndim != 3 or background_rgba.shape[-1] != 4:\n",
        "            raise ValueError(\"background_rgba must be (H,W,4)\")\n",
        "        if overlay_rgba.ndim != 3 or overlay_rgba.shape[-1] != 4:\n",
        "            raise ValueError(\"overlay_rgba must be (h,w,4)\")\n",
        "\n",
        "        H, W = background_rgba.shape[:2]\n",
        "        h, w = overlay_rgba.shape[:2]\n",
        "\n",
        "        if (h, w) != (H, W):\n",
        "            if not resize_overlay:\n",
        "                raise ValueError(\"Size mismatch; pass resize_overlay=True to auto-resize overlay.\")\n",
        "            overlay_rgba = cv2.resize(overlay_rgba, (W, H), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Convert to float32 in [0,1]\n",
        "        bg = background_rgba.astype(np.float32) / 255.0\n",
        "        fg = overlay_rgba.astype(np.float32) / 255.0\n",
        "\n",
        "        a_b = bg[..., 3:4]  # (H,W,1)\n",
        "        a_f = fg[..., 3:4]\n",
        "\n",
        "        if premultiplied:\n",
        "            # If inputs are premultiplied: colors already multiplied by alpha\n",
        "            # out.rgb = fg.rgb + (1 - a_f) * bg.rgb\n",
        "            # out.a   = a_f + (1 - a_f) * a_b\n",
        "            out_rgb = fg[..., :3] + (1.0 - a_f) * bg[..., :3]\n",
        "        else:\n",
        "            # Straight alpha\n",
        "            # out.rgb = (fg.rgb*a_f + bg.rgb*a_b*(1 - a_f)) / out.a   (but we usually keep straight result)\n",
        "            out_rgb = fg[..., :3] * a_f + bg[..., :3] * (1.0 - a_f)\n",
        "\n",
        "        out_a = a_f + (1.0 - a_f) * a_b\n",
        "\n",
        "        out = np.concatenate([out_rgb, out_a], axis=-1)\n",
        "        out = np.clip(out * 255.0, 0, 255).astype(np.uint8)\n",
        "        return out\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def mesh_wireframe_image(\n",
        "        mesh: Meshes,\n",
        "        cameras: CamerasBase,\n",
        "        image_size: Tuple[int, int] = (480, 640),         # (H, W)\n",
        "        edge_mode: Literal[\"all\", \"boundary\", \"feature\", \"boundary+feature\"] = \"boundary+feature\",\n",
        "        feature_deg: float = 30.0,\n",
        "        line_rgb: Tuple[int, int, int] = (0, 255, 0),     # RGB color for lines\n",
        "        line_thickness: int = 2,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Render a wireframe-only RGB image from a PyTorch3D mesh by projecting edges.\n",
        "\n",
        "        Args:\n",
        "            mesh: PyTorch3D Meshes (batch=1).\n",
        "            cameras: PyTorch3D camera (PerspectiveCameras/FoV...); must support transform_points_screen.\n",
        "            image_size: (H, W) in pixels.\n",
        "            edge_mode:\n",
        "                \"all\"               -> all unique triangle edges\n",
        "                \"boundary\"          -> only boundary (silhouette/topology) edges\n",
        "                \"feature\"           -> only edges with dihedral angle > feature_deg\n",
        "                \"boundary+feature\"  -> union of the above (default)\n",
        "            feature_deg: dihedral angle threshold for feature edges.\n",
        "            line_rgb: RGB color for wireframe.\n",
        "            line_thickness: line thickness in pixels.\n",
        "\n",
        "        Returns:\n",
        "            img_rgb: np.uint8 array of shape (H, W, 3) in RGB (ready for plt.imshow).\n",
        "        \"\"\"\n",
        "        # assert mesh.num_meshes() == 1, \"Provide a single mesh (batch=1).\"\n",
        "        device = mesh.device\n",
        "        H, W = image_size\n",
        "\n",
        "        verts = mesh.verts_packed()   # (V,3)\n",
        "        faces = mesh.faces_packed()   # (F,3)\n",
        "\n",
        "        # --- Build unique edge list ---\n",
        "        e01 = faces[:, [0, 1]]\n",
        "        e12 = faces[:, [1, 2]]\n",
        "        e20 = faces[:, [2, 0]]\n",
        "        edges = torch.cat([e01, e12, e20], dim=0)           # (3F,2)\n",
        "        edges = torch.sort(edges, dim=1).values             # canonicalize (i<j)\n",
        "        edges = torch.unique(edges, dim=0)                  # (E,2)\n",
        "\n",
        "        def edge_face_adjacency(faces_t: torch.Tensor):\n",
        "            e2f = {}\n",
        "            F = faces_t.shape[0]\n",
        "            for fid in range(F):\n",
        "                f = faces_t[fid].tolist()\n",
        "                for (a, b) in ((f[0], f[1]), (f[1], f[2]), (f[2], f[0])):\n",
        "                    i, j = (a, b) if a < b else (b, a)\n",
        "                    e2f.setdefault((i, j), []).append(fid)\n",
        "            return e2f  # map (i,j) -> [face_ids...]\n",
        "\n",
        "        # --- Select edges (boundary/feature) if requested ---\n",
        "        if edge_mode != \"all\":\n",
        "            e2f = edge_face_adjacency(faces)\n",
        "            select = []\n",
        "\n",
        "            want_boundary = (\"boundary\" in edge_mode)\n",
        "            want_feature  = (\"feature\"  in edge_mode)\n",
        "            if want_feature:\n",
        "                v0 = verts[faces[:, 0]]\n",
        "                v1 = verts[faces[:, 1]]\n",
        "                v2 = verts[faces[:, 2]]\n",
        "                fn = torch.nn.functional.normalize(torch.cross(v1 - v0, v2 - v0, dim=1), dim=1)  # (F,3)\n",
        "                cos_thresh = float(np.cos(np.deg2rad(feature_deg)))\n",
        "\n",
        "            for (i, j), fids in e2f.items():\n",
        "                add = False\n",
        "                if want_boundary and len(fids) == 1:\n",
        "                    add = True\n",
        "                if want_feature and len(fids) == 2:\n",
        "                    n0, n1 = fn[fids[0]], fn[fids[1]]\n",
        "                    cosang = torch.dot(n0, n1).clamp(-1, 1).item()\n",
        "                    if cosang < cos_thresh:\n",
        "                        add = True\n",
        "                if add:\n",
        "                    select.append((i, j))\n",
        "            if not select:  # fallback\n",
        "                select = [tuple(e.tolist()) for e in edges]\n",
        "            edges = torch.tensor(select, dtype=torch.int64, device=device)\n",
        "\n",
        "        # --- Project vertices to pixels ---\n",
        "        # transform_points_screen returns (B,N,3) with xy in pixels when in_ndc=False & image_size is given\n",
        "        verts_batched = verts.unsqueeze(0)  # (1,V,3)\n",
        "        verts_scr = cameras.transform_points_screen(verts_batched, image_size=((H, W),))[0, :, :2]  # (V,2)\n",
        "        # Also get camera-space Z to cull behind-camera points\n",
        "        to_view = cameras.get_world_to_view_transform()\n",
        "        verts_cam = to_view.transform_points(verts_batched)[0]  # (V,3)\n",
        "\n",
        "        # --- Draw on white canvas using OpenCV ---\n",
        "        img_bgr = np.full((H, W, 3), 255, dtype=np.uint8)  # white background (BGR)\n",
        "        # OpenCV expects BGR; convert our RGB color:\n",
        "        bgr = (int(line_rgb[2]), int(line_rgb[1]), int(line_rgb[0]))\n",
        "\n",
        "        v2d = verts_scr.detach().cpu().numpy()\n",
        "        zc  = verts_cam[:, 2].detach().cpu().numpy()\n",
        "\n",
        "        for i0, i1 in edges.detach().cpu().numpy():\n",
        "            # simple visibility: both endpoints in front of camera\n",
        "            if zc[i0] <= 0 or zc[i1] <= 0:\n",
        "                continue\n",
        "            u0, v0 = v2d[i0]\n",
        "            u1, v1 = v2d[i1]\n",
        "            # clip to image bounds (optional: skip if far off-screen)\n",
        "            if not (np.isfinite([u0, v0, u1, v1]).all()):\n",
        "                continue\n",
        "            p0 = (int(round(u0)), int(round(v0)))\n",
        "            p1 = (int(round(u1)), int(round(v1)))\n",
        "            # Draw if at least partially in the image\n",
        "            if (0 <= p0[0] < W or 0 <= p1[0] < W) and (0 <= p0[1] < H or 0 <= p1[1] < H):\n",
        "                cv2.line(img_bgr, p0, p1, color=bgr, thickness=line_thickness, lineType=cv2.LINE_AA)\n",
        "\n",
        "        # Convert BGR -> RGB for matplotlib\n",
        "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "        return img_rgb\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def contour_from_nonwhite_rgb(\n",
        "        rgb: np.ndarray,\n",
        "        *,\n",
        "        draw_color: Tuple[int,int,int] = (0,255,0),   # RGB for contour lines\n",
        "        thickness: int = 2,\n",
        "        mode: Literal[\"largest\",\"all\"] = \"all\",   # external: largest or all\n",
        "        white_tol: int = 0,                          # tolerance for \"white\" (per channel)\n",
        "        post_close: int = 3,                          # morph close kernel (0=skip)\n",
        "        fill_holes: bool = False,                      # fill interior holes to avoid inner contours\n",
        "        return_masks: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create contour overlay from a PyTorch3D RGB render by masking non-white pixels.\n",
        "\n",
        "        Args:\n",
        "            rgb: (H,W,3) float [0,1] or uint8 [0,255]\n",
        "            draw_color: RGB contour color\n",
        "            thickness: line thickness\n",
        "            mode: \"largest\" or \"all\" (external contours only)\n",
        "            white_tol: pixels with all channels >= 255-white_tol -> treated as background\n",
        "            post_close: morphology close kernel size (e.g., 3 or 5). 0 disables.\n",
        "            fill_holes: flood-fill background then invert to remove interior holes\n",
        "            return_masks: also return (mask, contour_only_img)\n",
        "\n",
        "        Returns:\n",
        "            contour_rgb (H,W,3) uint8\n",
        "            [optional] mask (H,W) uint8 in {0,255}\n",
        "            [optional] contour_only (H,W,3) uint8 on white bg\n",
        "        \"\"\"\n",
        "        if rgb.ndim != 3 or rgb.shape[2] != 3:\n",
        "            raise ValueError(\"rgb must be (H,W,3)\")\n",
        "\n",
        "        # Normalize to uint8\n",
        "        if np.issubdtype(rgb.dtype, np.floating):\n",
        "            img = (np.clip(rgb, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
        "        else:\n",
        "            img = np.clip(rgb, 0, 255).astype(np.uint8)\n",
        "\n",
        "        H, W = img.shape[:2]\n",
        "\n",
        "        # 1) Foreground mask = non-white pixels\n",
        "        # nonwhite if ANY channel < 255 - tol\n",
        "        nonwhite = (img < (255 - white_tol)).any(axis=2)\n",
        "        mask = (nonwhite.astype(np.uint8) * 255)  # (H,W) {0,255}\n",
        "\n",
        "        # 2) Optional cleanup: close small gaps on edges\n",
        "        if post_close and post_close > 0:\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (post_close, post_close))\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k)\n",
        "\n",
        "        # 3) Optional: fill holes so internal contours disappear\n",
        "        if fill_holes:\n",
        "            # flood fill from border to get background, invert to keep filled foreground\n",
        "            ff = mask.copy()\n",
        "            h, w = ff.shape\n",
        "            ff_pad = np.pad(ff, ((1,1),(1,1)), mode='constant', constant_values=0)\n",
        "            mask_filled = ff_pad.copy()\n",
        "            cv2.floodFill(mask_filled, None, (0,0), 255)             # fill background outside object\n",
        "            mask_bg = (mask_filled == 255)[1:-1,1:-1]                # remove pad\n",
        "            # foreground = original OR NOT background-fill\n",
        "            mask = np.where(mask_bg, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # 4) External contours only (prevents inner contours)\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not contours:\n",
        "            contour_rgb = img.copy()\n",
        "            if return_masks:\n",
        "                return contour_rgb, mask, np.full_like(img, 255)\n",
        "            return contour_rgb\n",
        "\n",
        "        if mode == \"largest\":\n",
        "            contours = [max(contours, key=cv2.contourArea)]\n",
        "\n",
        "        # 5) Draw contours on original (OpenCV uses BGR)\n",
        "        contour_rgb_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        bgr = (draw_color[2], draw_color[1], draw_color[0])\n",
        "        cv2.drawContours(contour_rgb_bgr, contours, -1, bgr, thickness)\n",
        "        contour_rgb = cv2.cvtColor(contour_rgb_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if return_masks:\n",
        "            contour_only = np.full_like(img, 255)\n",
        "            c_bgr = cv2.cvtColor(contour_only, cv2.COLOR_RGB2BGR)\n",
        "            cv2.drawContours(c_bgr, contours, -1, bgr, thickness)\n",
        "            contour_only = cv2.cvtColor(c_bgr, cv2.COLOR_BGR2RGB)\n",
        "            return contour_rgb, mask, contour_only\n",
        "\n",
        "        return contour_rgb\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def contour_from_nonwhite_rgb_semi_transparent(\n",
        "        rgb: np.ndarray,\n",
        "        *,\n",
        "        draw_color: Tuple[int,int,int] = (0,255,0),   # RGB for contour lines\n",
        "        thickness: int = 2,\n",
        "        mode: Literal[\"largest\",\"all\"] = \"all\",       # external: largest or all\n",
        "        white_tol: int = 0,                           # tolerance for \"white\" (per channel)\n",
        "        post_close: int = 3,                          # morph close kernel (0=skip)\n",
        "        fill_holes: bool = False,                     # fill interior holes to avoid inner contours\n",
        "        return_masks: bool = False,\n",
        "        return_rgba: bool = False,                    # NEW: if True, return RGBA image instead of RGB\n",
        "        alpha_value: float = 0.5                      # alpha value for the non-contour region\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create contour overlay from a PyTorch3D RGB render by masking non-white pixels.\n",
        "\n",
        "        Args:\n",
        "            rgb: (H,W,3) float [0,1] or uint8 [0,255]\n",
        "            draw_color: RGB contour color\n",
        "            thickness: line thickness\n",
        "            mode: \"largest\" or \"all\" (external contours only)\n",
        "            white_tol: pixels with all channels >= 255-white_tol -> treated as background\n",
        "            post_close: morphology close kernel size (e.g., 3 or 5). 0 disables.\n",
        "            fill_holes: flood-fill background then invert to remove interior holes\n",
        "            return_masks: also return (mask, contour_only_img)\n",
        "            return_rgba: if True, output is (H,W,4) with alpha channel\n",
        "            alpha_value: alpha assigned to non-contour pixels [0..1]\n",
        "\n",
        "        Returns:\n",
        "            contour_img (H,W,3) or (H,W,4) uint8\n",
        "            [optional] mask (H,W) uint8 in {0,255}\n",
        "            [optional] contour_only (H,W,3) uint8\n",
        "        \"\"\"\n",
        "        if rgb.ndim != 3 or rgb.shape[2] != 3:\n",
        "            raise ValueError(\"rgb must be (H,W,3)\")\n",
        "\n",
        "        # Normalize to uint8\n",
        "        if np.issubdtype(rgb.dtype, np.floating):\n",
        "            img = (np.clip(rgb, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
        "        else:\n",
        "            img = np.clip(rgb, 0, 255).astype(np.uint8)\n",
        "\n",
        "        H, W = img.shape[:2]\n",
        "\n",
        "        # 1) Foreground mask = non-white pixels\n",
        "        nonwhite = (img < (255 - white_tol)).any(axis=2)\n",
        "        mask = (nonwhite.astype(np.uint8) * 255)  # (H,W)\n",
        "\n",
        "        # 2) Cleanup\n",
        "        if post_close and post_close > 0:\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (post_close, post_close))\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, k)\n",
        "\n",
        "        # 3) Fill holes (optional)\n",
        "        if fill_holes:\n",
        "            ff = mask.copy()\n",
        "            ff_pad = np.pad(ff, ((1,1),(1,1)), mode='constant', constant_values=0)\n",
        "            cv2.floodFill(ff_pad, None, (0,0), 255)\n",
        "            mask_bg = (ff_pad == 255)[1:-1,1:-1]\n",
        "            mask = np.where(mask_bg, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # 4) Find contours\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if not contours:\n",
        "            if return_rgba:\n",
        "                rgba = np.dstack([img, np.full((H,W), int(alpha_value*255), np.uint8)])\n",
        "                return rgba\n",
        "            if return_masks:\n",
        "                return img, mask, np.full_like(img, 255)\n",
        "            return img\n",
        "\n",
        "        if mode == \"largest\":\n",
        "            contours = [max(contours, key=cv2.contourArea)]\n",
        "\n",
        "        # 5) Prepare output\n",
        "        contour_rgb_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        bgr = (draw_color[2], draw_color[1], draw_color[0])\n",
        "        cv2.drawContours(contour_rgb_bgr, contours, -1, bgr, thickness)\n",
        "        contour_rgb = cv2.cvtColor(contour_rgb_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if return_rgba:\n",
        "            # Build alpha: everything gets alpha_value, contour pixels forced to 1\n",
        "            alpha = np.full((H, W), int(alpha_value*255), dtype=np.uint8)\n",
        "            contour_mask = np.zeros((H,W), dtype=np.uint8)\n",
        "            cv2.drawContours(contour_mask, contours, -1, 255, thickness)\n",
        "            alpha[contour_mask > 0] = 255  # contours fully opaque\n",
        "            contour_rgba = np.dstack([contour_rgb, alpha])\n",
        "            return contour_rgba\n",
        "\n",
        "        if return_masks:\n",
        "            contour_only = np.full_like(img, 255)\n",
        "            c_bgr = cv2.cvtColor(contour_only, cv2.COLOR_RGB2BGR)\n",
        "            cv2.drawContours(c_bgr, contours, -1, bgr, thickness)\n",
        "            contour_only = cv2.cvtColor(c_bgr, cv2.COLOR_BGR2RGB)\n",
        "            return contour_rgb, mask, contour_only\n",
        "\n",
        "        return contour_rgb\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def depth_to_rgb(\n",
        "        depth: torch.Tensor,\n",
        "        cmap: str = \"viridis\",\n",
        "        bg_mode: Literal[\"black\",\"white\",\"transparent\"] = \"black\"\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert a PyTorch3D depth map into a visualization image.\n",
        "        Automatically treats depth = -1 as invalid background.\n",
        "\n",
        "        Args:\n",
        "            depth: (H,W) torch.Tensor of depth values (float).\n",
        "            cmap:  Matplotlib colormap name (default: \"viridis\").\n",
        "            bg_mode:\n",
        "                \"black\"       -> background stays black (default)\n",
        "                \"white\"       -> background set to white\n",
        "                \"transparent\" -> return RGBA image with alpha=0 for background\n",
        "\n",
        "        Returns:\n",
        "            img: (H,W,3) uint8 RGB or (H,W,4) uint8 RGBA if bg_mode=\"transparent\"\n",
        "        \"\"\"\n",
        "        if not isinstance(depth, torch.Tensor):\n",
        "            raise ValueError(\"depth must be a torch.Tensor\")\n",
        "\n",
        "        depth_np = depth.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # Mask out invalid regions (zbuf = -1 means background in PyTorch3D)\n",
        "        valid_mask = depth_np > 0\n",
        "\n",
        "        if np.any(valid_mask):\n",
        "            dmin, dmax = depth_np[valid_mask].min(), depth_np[valid_mask].max()\n",
        "            depth_norm = np.zeros_like(depth_np, dtype=np.float32)\n",
        "            depth_norm[valid_mask] = (depth_np[valid_mask] - dmin) / (dmax - dmin + 1e-8)\n",
        "        else:\n",
        "            depth_norm = np.zeros_like(depth_np, dtype=np.float32)\n",
        "\n",
        "        cmap_func = cm.get_cmap(cmap)\n",
        "        rgba = cmap_func(depth_norm)  # (H,W,4) floats in [0,1]\n",
        "\n",
        "        if bg_mode == \"transparent\":\n",
        "            rgba[..., 3] = 0.0          # alpha=0 where background\n",
        "            rgba[valid_mask, 3] = 1.0   # alpha=1 where valid\n",
        "            img = (rgba * 255).astype(np.uint8)  # RGBA\n",
        "        else:\n",
        "            rgb = (rgba[..., :3] * 255).astype(np.uint8)\n",
        "            if bg_mode == \"white\":\n",
        "                rgb[~valid_mask] = [255,255,255]\n",
        "            elif bg_mode == \"black\":\n",
        "                rgb[~valid_mask] = [0,0,0]\n",
        "            img = rgb\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def make_binary_mask(img: np.ndarray, white_tol: int = 5, black_tol: int = 5) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create a binary mask (1=foreground, 0=background) from an RGB image\n",
        "        where the background can be either black or white.\n",
        "\n",
        "        Args:\n",
        "            img: (H,W,3) uint8 or float image.\n",
        "            white_tol: tolerance for detecting white background (0–255).\n",
        "            black_tol: tolerance for detecting black background (0–255).\n",
        "\n",
        "        Returns:\n",
        "            mask: (H,W) np.uint8 with {0,1}\n",
        "        \"\"\"\n",
        "        if img.ndim != 3 or img.shape[2] != 3:\n",
        "            raise ValueError(\"Expected an RGB image with shape (H,W,3).\")\n",
        "\n",
        "        # Normalize to uint8 if in float\n",
        "        if np.issubdtype(img.dtype, np.floating):\n",
        "            if img.max() <= 1.0:\n",
        "                img_u8 = (img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img_u8 = img.astype(np.uint8)\n",
        "        else:\n",
        "            img_u8 = img.copy()\n",
        "\n",
        "        H, W, _ = img_u8.shape\n",
        "\n",
        "        # Look at border pixels to decide background type\n",
        "        border = np.concatenate([img_u8[0,:,:], img_u8[-1,:,:], img_u8[:,0,:], img_u8[:,-1,:]], axis=0)\n",
        "        border_mean = border.mean()\n",
        "\n",
        "        if border_mean < 127:\n",
        "            # Background is black\n",
        "            mask = (np.any(img_u8 > black_tol, axis=-1)).astype(np.uint8)\n",
        "        else:\n",
        "            # Background is white\n",
        "            mask = (np.any(img_u8 < 255 - white_tol, axis=-1)).astype(np.uint8)\n",
        "\n",
        "        return mask\n",
        "\n",
        "\n",
        "\n",
        "class RenderWithPytorch3D:\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def render_rgb_depth_from_view_from_RT(\n",
        "        mesh: Meshes,\n",
        "        *,\n",
        "        fx: float, fy: float, cx: float, cy: float,\n",
        "        width: int, height: int,\n",
        "        # distance: float, elev: float, azim: float, roll_deg: float = 0.0,\n",
        "        # roll_mode: str = \"world\",                  # \"camera\" or \"world\"\n",
        "        R: Optional[torch.Tensor]  | None = None,          # (1,3,3)\n",
        "        T: Optional[torch.Tensor]  | None = None,          # (1,3)\n",
        "        raster_settings: RasterizationSettings | None = None,\n",
        "        lights: PointLights | None = None,\n",
        "        device: torch.device | None = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Render an RGB image and a depth map from a PyTorch3D mesh at a given camera view + roll.\n",
        "\n",
        "        Returns:\n",
        "            rgb_np   : (H,W,3) float32 in [0,1]\n",
        "            depth_t  : (H,W)   torch.float32, metric Z in camera coords; invalid pixels == -1\n",
        "            cameras  : the PerspectiveCameras used (in case you want to reuse)\n",
        "        \"\"\"\n",
        "        device = device or mesh.device\n",
        "\n",
        "\n",
        "        # # 1) Base view (look-at origin) from spherical params\n",
        "        # R, T = look_at_view_transform(dist=distance, elev=elev, azim=azim, device=device)\n",
        "\n",
        "        # # 2) Add in-plane roll\n",
        "        # R, T = add_camera_roll_to_RT(R, T, roll_deg=roll_deg, device=device, mode=roll_mode)\n",
        "\n",
        "        # print(\"Rotation inside function: \\n\", R)\n",
        "        # print(\"Translation inside T: \\n\", T)\n",
        "\n",
        "\n",
        "\n",
        "        # 3) Camera with pixel intrinsics (OpenCV-like) and in_ndc=False\n",
        "        cameras = PerspectiveCameras(\n",
        "            focal_length=torch.tensor([[fx, fy]], dtype=torch.float32, device=device),\n",
        "            principal_point=torch.tensor([[cx, cy]], dtype=torch.float32, device=device),\n",
        "            R=R, T=T,\n",
        "            image_size=torch.tensor([[height, width]], dtype=torch.float32, device=device),\n",
        "            in_ndc=False, device=device\n",
        "        )\n",
        "\n",
        "        # 4) Default raster/shader if none provided\n",
        "        if raster_settings is None:\n",
        "            raster_settings = RasterizationSettings(\n",
        "                image_size=(height, width),\n",
        "                blur_radius=0.0,\n",
        "                faces_per_pixel=1\n",
        "            )\n",
        "        if lights is None:\n",
        "            lights = PointLights(device=device, location=[[2.0, 2.0, 2.0]])\n",
        "\n",
        "        renderer = MeshRenderer(\n",
        "            rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "            shader=SoftPhongShader(device=device, cameras=cameras, lights=lights),\n",
        "        )\n",
        "\n",
        "        # 5) Render RGB\n",
        "        # images = renderer(mesh, cameras=cameras, lights=lights)          # (1,H,W,4)\n",
        "        images = renderer(mesh)          # (1,H,W,4)\n",
        "        rgb_np = images[0, ..., :3].detach().cpu().numpy()               # (H,W,3) float in [0,1]\n",
        "\n",
        "        # 6) Depth (metric Z; background == -1)\n",
        "        # fragments = renderer.rasterizer(mesh, cameras=cameras)\n",
        "        fragments = renderer.rasterizer(mesh)\n",
        "        depth_t = fragments.zbuf[0, ..., 0].detach()                     # (H,W) torch.float32\n",
        "\n",
        "        return rgb_np, depth_t, cameras\n",
        "\n",
        "\n",
        "    # --- Helper: add an in-plane roll to a PyTorch3D camera (R, T) ---\n",
        "    @staticmethod\n",
        "    def add_camera_roll_to_RT(R, T, roll_deg: float, device=None, mode: str = \"world\"):\n",
        "        \"\"\"\n",
        "        Add a roll (rotation about the camera's viewing axis) to (R, T).\n",
        "\n",
        "        Args:\n",
        "            R, T: PyTorch3D extrinsics (as from look_at_view_transform), shapes (1,3,3), (1,3)\n",
        "            roll_deg: roll angle in degrees, positive = CCW in image plane\n",
        "            device: torch device\n",
        "            mode:\n",
        "              - \"camera\": pre-multiply in camera space  (R' = Rroll @ R)\n",
        "              - \"world\" : post-multiply in world space  (R' = R @ Rroll)\n",
        "                For PyTorch3D’s convention (X_cam = X_world @ R^T + T), \"world\" is often intuitive.\n",
        "\n",
        "        Returns:\n",
        "            (R_rolled, T)  (T unchanged)\n",
        "        \"\"\"\n",
        "        device = device or R.device\n",
        "        th = math.radians(roll_deg)\n",
        "        c, s = math.cos(th), math.sin(th)\n",
        "        Rroll = torch.tensor([[ c, -s, 0.0],\n",
        "                              [ s,  c, 0.0],\n",
        "                              [0.0, 0.0, 1.0]], dtype=R.dtype, device=device).unsqueeze(0)  # (1,3,3)\n",
        "\n",
        "        if mode == \"camera\":\n",
        "            R_new = torch.bmm(Rroll, R)     # (1,3,3)\n",
        "        elif mode == \"world\":\n",
        "            R_new = torch.bmm(R, Rroll)     # (1,3,3)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'camera' or 'world'\")\n",
        "        return R_new, T\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def render_rgb_depth_from_view(\n",
        "        mesh: Meshes,\n",
        "        *,\n",
        "        fx: float, fy: float, cx: float, cy: float,\n",
        "        width: int, height: int,\n",
        "        distance: float, elev: float, azim: float, roll_deg: float = 0.0,\n",
        "        roll_mode: str = \"world\",                  # \"camera\" or \"world\"\n",
        "        raster_settings: RasterizationSettings | None = None,\n",
        "        lights: PointLights | None = None,\n",
        "        device: torch.device | None = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Render an RGB image and a depth map from a PyTorch3D mesh at a given camera view + roll.\n",
        "\n",
        "        Returns:\n",
        "            rgb_np   : (H,W,3) float32 in [0,1]\n",
        "            depth_t  : (H,W)   torch.float32, metric Z in camera coords; invalid pixels == -1\n",
        "            cameras  : the PerspectiveCameras used (in case you want to reuse)\n",
        "        \"\"\"\n",
        "        device = device or mesh.device\n",
        "\n",
        "        # 1) Base view (look-at origin) from spherical params\n",
        "        R, T = look_at_view_transform(dist=distance, elev=elev, azim=azim, device=device)\n",
        "\n",
        "        # 2) Add in-plane roll\n",
        "        R, T = RenderWithPytorch3D.add_camera_roll_to_RT(R, T, roll_deg=roll_deg, device=device, mode=roll_mode)\n",
        "\n",
        "        # print(\"Rotation inside function: \\n\", R)\n",
        "        # print(\"Translation inside T: \\n\", T)\n",
        "\n",
        "\n",
        "        # 3) Camera with pixel intrinsics (OpenCV-like) and in_ndc=False\n",
        "        cameras = PerspectiveCameras(\n",
        "            focal_length=torch.tensor([[fx, fy]], dtype=torch.float32, device=device),\n",
        "            principal_point=torch.tensor([[cx, cy]], dtype=torch.float32, device=device),\n",
        "            R=R, T=T,\n",
        "            image_size=torch.tensor([[height, width]], dtype=torch.float32, device=device),\n",
        "            in_ndc=False, device=device\n",
        "        )\n",
        "\n",
        "        # 4) Default raster/shader if none provided\n",
        "        if raster_settings is None:\n",
        "            raster_settings = RasterizationSettings(\n",
        "                image_size=(height, width),\n",
        "                blur_radius=0.0,\n",
        "                faces_per_pixel=1,\n",
        "                perspective_correct = True\n",
        "            )\n",
        "        if lights is None:\n",
        "            lights = PointLights(device=device, location=[[2.0, 2.0, 2.0]])\n",
        "\n",
        "        renderer = MeshRenderer(\n",
        "            rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "            shader=SoftPhongShader(device=device, cameras=cameras, lights=lights),\n",
        "        )\n",
        "\n",
        "        # # Sanity: how does the camera map two simple world points?\n",
        "        # H, W = height, width\n",
        "        # test = torch.tensor([  # world points\n",
        "        #     [ 0.0, 0.0, 0.0],   # origin\n",
        "        #     [ 0.1, 0.0, 0.0],   # +x_world\n",
        "        #     [-0.1, 0.0, 0.0],   # -x_world\n",
        "        #     [ 0.0, 0.1, 0.0],   # +y_world\n",
        "        #     [ 0.0,-0.1, 0.0],   # -y_world\n",
        "        # ], device=device).unsqueeze(0)  # (1,5,3)\n",
        "\n",
        "        # uvz = cameras.transform_points_screen(\n",
        "        #     test, image_size=torch.tensor([[H, W]], device=device)\n",
        "        # )[0]  # (5,3)\n",
        "\n",
        "        # print(\"u(+x_world) =\", float(uvz[1,0]), \"  u(-x_world) =\", float(uvz[2,0]), \"  cx ~\", float(cameras.principal_point[0,0]))\n",
        "        # print(\"v(+y_world) =\", float(uvz[3,1]), \"  v(-y_world) =\", float(uvz[4,1]), \"  cy ~\", float(cameras.principal_point[0,1]))\n",
        "\n",
        "        # print(\"focal_length=\", cameras.focal_length[0])  # <-- check sign of fx, fy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # 5) Render RGB\n",
        "        # images = renderer(mesh, cameras=cameras, lights=lights)          # (1,H,W,4)\n",
        "        images = renderer(mesh)          # (1,H,W,4)\n",
        "        rgb_np = images[0, ..., :3].detach().cpu().numpy()               # (H,W,3) float in [0,1]\n",
        "\n",
        "        # 6) Depth (metric Z; background == -1)\n",
        "        # fragments = renderer.rasterizer(mesh, cameras=cameras)\n",
        "        fragments = renderer.rasterizer(mesh)\n",
        "        depth_t = fragments.zbuf[0, ..., 0].detach()                     # (H,W) torch.float32\n",
        "\n",
        "        return rgb_np, depth_t, cameras\n",
        "\n",
        "\n",
        "\n",
        "def mast3r_match_features(template_image, cams_template, depth_template, target_image, model):\n",
        "\n",
        "\n",
        "    # We need these for later in the program\n",
        "    device = cams_template.R.device\n",
        "    dtype  = cams_template.R.dtype\n",
        "    # imgsz  = torch.tensor([[H, W]], device=device)\n",
        "\n",
        "    # Save rgb image to file for now to test feature detection directly.\n",
        "    # TODO: Change load_images function or create a new version that accepts image arrays instead of file names/directories\n",
        "    # img is a numpy array (H,W,3)\n",
        "    plt.imsave(path_to_images + \"template_image.png\", template_image)\n",
        "\n",
        "    # Save rgb image to file for now to test feature detection directly.\n",
        "    # Change load_images function or create a new version that accepts image arrays instead of file names/directories\n",
        "    # img is a numpy array (H,W,3)\n",
        "    plt.imsave(path_to_images + \"target_image.png\", target_image)\n",
        "\n",
        "    # Load image into list to create input for feature matcher\n",
        "    images = load_images([path_to_images + 'template_image.png', path_to_images + 'target_image.png'], size=256, square_ok=True)\n",
        "    print(images[0][\"img\"].shape)\n",
        "\n",
        "    print(\"***-------------------------------------------------------------- ***\")\n",
        "    print(\"**  3. Feature matching                                            **\")\n",
        "    print(\"***-------------------------------------------------------------- ***\")\n",
        "\n",
        "    # Inference\n",
        "    matches_template, matches_target, \\\n",
        "    view_template, pred_template, \\\n",
        "    view_target, pred_target = featmatchtools.FeatureMatcher.mast3r_inference(images = images,\n",
        "                                                              model = model,\n",
        "                                                              device = device)\n",
        "\n",
        "    # convert the view output from mast3r to standard rgb image\n",
        "    v_template = featmatchtools.FeatureMatcher.mast3r_view2rgbimage(view_template)\n",
        "    v_target   = featmatchtools.FeatureMatcher.mast3r_view2rgbimage(view_target)\n",
        "\n",
        "    # Create binary mask for this view (used for filtering spurious matches)\n",
        "    v_template_mask = unproject_tools.ImageProcessor.make_binary_mask(v_template, white_tol= 0, black_tol = 0)  # returns 0/1 mask\n",
        "    v_template_mask = featmatchtools.FeatureMatcher.smooth_and_fill_mask(v_template_mask)\n",
        "\n",
        "    # Create a binary mask for this view (used for filtering spurious matches)\n",
        "    v_target_mask = unproject_tools.ImageProcessor.make_binary_mask(v_target, white_tol= 0, black_tol = 0)  # returns 0/1 mask\n",
        "    v_target_mask = featmatchtools.FeatureMatcher.smooth_and_fill_mask(v_target_mask)\n",
        "\n",
        "    # Obtain a clean set of matches for each image (no matches in background)\n",
        "    matches_template_fg, matches_target_fg = featmatchtools.FeatureMatcher.filter_matches_by_mask(matches_template, matches_target, v_template_mask, v_target_mask)\n",
        "\n",
        "    # These (u,v) points are on the template image (coarse pose).\n",
        "    # They are the input to the 3-D unprojector from depth\n",
        "    points_uv_template = matches_template_fg\n",
        "\n",
        "    # Calculate depths for each (u,v) template point. We do not use this function\n",
        "    # to calculate 3-D coordinates or camera coordinates. However, we keep X_world\n",
        "    # for now, as we need it to pass to the clean-up function in the next step.\n",
        "    # We ignore camera coordinates as we use PyTorch3D's unproject_points() for\n",
        "    # the actual recovery of 3-D points from depth.\n",
        "    fx = cams_template.focal_length.cpu().numpy().squeeze()[0]\n",
        "    fy = cams_template.focal_length.cpu().numpy().squeeze()[1]\n",
        "    cx = cams_template.principal_point.cpu().numpy().squeeze()[0]\n",
        "    cy = cams_template.principal_point.cpu().numpy().squeeze()[1]\n",
        "    _, X_world, depths = \\\n",
        "      unproject_tools.Unprojector.recover_3D_points(matches_template_fg,\n",
        "                                                    depth_template.cpu(),\n",
        "                                                    fx, fy, cx, cy,\n",
        "                                                    cams_template.R,\n",
        "                                                    cams_template.T)\n",
        "\n",
        "    # Remove the matches for which 3-D reprojected coordinates have nan\n",
        "    # uv_clean are corresponding points to the actual test image (i.e., cutout image)\n",
        "    _, _, kept_idx, removed_idx = featmatchtools.FeatureMatcher.filter_nan_points(X_world, matches_template_fg)\n",
        "\n",
        "\n",
        "    template_points = matches_template_fg[kept_idx]\n",
        "    target_points = matches_target_fg[kept_idx]\n",
        "\n",
        "    return template_points, target_points, view_template, view_target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demo function to use for tests\n",
        "def create_and_display_image(distance=3,\n",
        "                             elev=0,\n",
        "                             azim=0,\n",
        "                             roll=0,\n",
        "                             K=np.eye(3),\n",
        "                             H=256,\n",
        "                             W=256):\n",
        "\n",
        "    # Get intrinsics\n",
        "    fx = K[0,0]\n",
        "    fy = K[1,1]\n",
        "    cx = K[0,2]\n",
        "    cy = K[1,2]\n",
        "\n",
        "    # Create rgb and depth images\n",
        "    rgb, depth, cams = unproject_tools.RenderWithPytorch3D.render_rgb_depth_from_view(\n",
        "        mesh,\n",
        "        fx=fx, fy=fy, cx=cx, cy=cy,\n",
        "        width=W, height=H,\n",
        "        distance=distance, elev=elev, azim=azim, roll_deg=roll,\n",
        "        roll_mode=\"camera\",   # try \"camera\" if you prefer or \"world\".\n",
        "    )\n",
        "\n",
        "    # Show\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1); plt.imshow(np.clip(rgb,0,1)); plt.axis('off'); plt.title('RGB')\n",
        "\n",
        "    # Depth visualization (treat -1 as invalid)\n",
        "    vis = unproject_tools.ImageProcessor.depth_to_rgb(depth, cmap=\"plasma\", bg_mode=\"white\")\n",
        "\n",
        "    plt.subplot(1,2,2); plt.imshow(vis); plt.axis('off'); plt.title('Depth')\n",
        "    plt.show()\n",
        "\n",
        "    #------------------------------- Show results ---------------------------------\n",
        "    myp3dtools.overlay_axes_p3d(rgb, cams, 256, 256,\n",
        "                    world_origin=(0,0,0), axis_len=0.3,\n",
        "                    draw_world_axes=True, draw_camera_axes=False,\n",
        "                    cam_axis_len=0.5,\n",
        "                    title=\"PyTorch3D camera\")\n",
        "\n",
        "    # Pretty print camera information\n",
        "    myp3dtools.print_camera_pose_matrices(cams.R, cams.T, \"*** PyTorch3D Camera ***\")\n",
        "\n",
        "    # clear gpu cache\n",
        "    unproject_tools.Util.clear_cuda_cache()\n",
        "\n",
        "    return rgb, depth, cams"
      ],
      "metadata": {
        "id": "kFO8Cxdrr58t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}